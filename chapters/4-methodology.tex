\section{Methodology}

In developing an effective methodology for analyzing memory consumption in seismic data processing using Python on Linux-based supercomputers, our approach focused on addressing the inherent complexities of Python as a high-level language.
Python's management of memory is not straightforward due to its layered architecture involving memory allocation, garbage collection, and potential interactions with C-based libraries and the operating system's memory management system.

\subsection{Memory Management Techniques}

\DF{Aqui precisamos construir um experimento formal que compare e mostre as diferenças em cada ferramenta}

The first phase of the research centered around establishing a reliable framework for measuring memory consumption.
Evaluating various tools and techniques was necessary to achieve this, including: mprof\footurl{https://github.com/pythonprofilers/memory\_profiler}, psutil\footurl{https://github.com/giampaolo/psutil}, and resource\footurl{https://docs.python.org/3/library/resource.html} modules.
These tools are particularly suited for Python environments, offering insights into dynamic memory allocation and real-time memory utilization.
However, considering the high-level nature of Python and its abstraction from direct memory management, it was necessary to supplement these tools with direct measurements from the Linux /proc\footurl{https://man7.org/linux/man-pages/man5/proc.5.html} filesystem, specifically using the smaps interface.
This approach allowed to obtain a more granular understanding of memory usage, which is essential when dealing with large datasets typically encountered in seismic data processing.

\subsection{Seismic Data Iteration Design}

The primary objective of this research was to gather a comprehensive profile of each seismic attribute's memory consumption.
To achieve this, it was essential to execute the same attribute on different shapes of seismic datasets.
Understanding the nature of seismic datasets is crucial in this context.
Typically, seismic data is three-dimensional, encompassing dimensions like inline, crossline, and time or depth slices.
These dimensions represent different aspects of the seismic survey grid, with inline and crossline denoting horizontal measurements and the third dimension representing vertical depth or time.

For the purpose of this research, we adopted a strategic approach to vary the dimensions of the seismic datasets while keeping a consistent structure.
Specifically, the latter two dimensions of the dataset, often corresponding to crossline and time/depth, were fixed at a size of 200.
This decision was made to maintain a level of consistency across different iterations, allowing for a more focused analysis on the impact of varying the first dimension - typically the inline dimension.

The variation in the first dimension was systematically implemented in steps of 200.
This approach enabled to explore a range of dataset sizes, starting from a relatively small dataset and progressively moving towards larger and more complex ones.
By incrementing the size in a controlled manner, it was possible to meticulously observe the changes in memory consumption and understand how scalability and dataset dimensions affect the memory footprint of seismic data processing.

Each iteration of the experiment, therefore, involved executing the same seismic attribute processing task on a dataset whose first dimension varied in size, while the other two dimensions remained constant.
This iterative process provided a detailed and nuanced profile of memory consumption across various dataset sizes, offering valuable insights into the efficiency and scalability of seismic data processing algorithms in high-performance computing environments.
Through this detailed iteration design, the study aimed to establish a clear understanding of how different dataset shapes influence memory usage, which is a critical consideration in optimizing seismic data processing tasks for large-scale geological explorations.

\DF{Aqui talvez seja bom uma imagem simples, mostrando de uma visão geral a questão das iterações}
\DF{Um ponto de dúvida aqui é: não fizemos um experimento formal que mostre que variando a D2 e D3 não muda. Eu rodei isso mas não tem o experimento desenhado. Vale ter isso? No caso pra mostrar que variar a inline da na mesma que variiar crossline our tempo do ponto de vista de memória}

\subsection{Experiment Design}

The foundational structure for all experiments in this study follows a standardized and replicable process, designed to ensure consistency and accuracy in the data collection.
This structure is centered around the use of a shell script that initiates a Docker\footurl{https://www.docker.com/} container for each iteration of the experiment.
An iteration here refers to a single execution of the seismic data processing task with a specific input data shape.

This process is divided into four distinct phases:
(i) on the first phase, a shell script initializes the Docker container to execute an iteration for a given data shape and seismic attribute;
(ii) the second phase happens within the Docker container itself, where a synthetic dataset is tailored to the specific iteration based on the expected input data shape;
(iii) the third phase involves the execution of the seismic attribute processing task on the synthetic dataset; and
(iv) the fourth and final phase is getting a summary of the result after all iterations are executed.
\DF{Aqui acho que vale colocar uma imagem ilustrando o processo}

It is important to notice that this containerized approach ensures a controlled and isolated environment for each experiment, minimizing external influences on the memory consumption data.
Also, the analysis of various memory management techniques conducted in the initial stages of this research made it clear that utilizing the /proc filesystem, specifically the smaps\_rollup file, offered a more accurate and detailed understanding of memory consumption.
Based on this, the containerized execution of each iteration is designed to pause and wait for a \textit{CONT} signal\footurl{https://man7.org/linux/man-pages/man7/signal.7.html} on three specific phases: before phase (ii), before phase (iii), and after phase (iii).
The shell script watches the execution of the experiment, and fetches the memory consumption data from the /proc filesystem at each pause, sending the \textit{CONT} signal to the container to continue the execution.
With this approach it is possible to get a detailed memory consumption profile for each iteration, including the memory consumption of the data generation phase and the seismic attribute execution phase.

This structured approach to executing each iteration of the experiment allows for a systematic and detailed collection of memory usage data.
Controlling the execution flow and precisely timing the memory measurements ensures that the data accurately reflects the memory consumption patterns at each stage of the seismic data processing task.
This methodology is critical in enabling a deep understanding of how different input data shapes and processing tasks influence overall memory usage in a high-performance computing environment.

\DF{Em algum lugar preciso colocar um link pro repositório com os experimentos}

% Falta
% - Falar do primeiro experimento, pra avaliar o consumo de memória dos atributos
% - Falar do segundo, que foi pra avaliar a pressão de memória
% - Falar do terceiro, que foi pra avaliar a diferença usando dados reais

% - O que fizemos:
%   - REFAZER Avaliamos a diferença na forma de medir o consumo de memória
%       - Esse aqui a gente não fez formalmente, sinto que falta um breve experimento
%       - Medir as seguintes formas:
%         - Usando o mprof
%         - Usando o psutil
%         - Usando a biblioteca resource
%         - Usando o proc do Linux direto
%       - Nesse ponto, falar sobre a dificuldade de medir quando estamos falando de múltiplos processos
%   - Medimos o perfil do consumo de memória para todos os atributos usando um único processo
%   - Avaliamos o perfil do consumo de memória para alguns atributos quando colocamos pressão de memória
%   - Testamos a diferença no consumo de memória quando usamos dados sintéticos e descobrimos uma grande discrepância
%   - FALTA FAZER Testar se essa diferença continua acontecendo se salvamos os dados em disco
% 
% - Section structure
%   - In order to create an accurate model for a given algorithm we need an accurate way of measuring memory consumption
%   - Seismic data is large, and usually we use Python to process it
%   - We also use supercomputers due to the size of the data, and those supercomputers uses Linux
%   - Since Python is a high level language, we have many layers of optimization and abstraction between the actual code and allocating the memory itself
%   - Give a few examples of those layers, like: how Python handles memory allocation and garbage collection, the possibility of libraries using C, and the OS itself
%   - Explain the different ways that we can evaluate memory consumption using Python. Examples: mprof, psutil and resource
%   - Besides every way Python supports measuring memory consumption, we also can measure it using the values from the /proc filesystem
%   - Explain the way we organized all experiments
%     - We split into two separate components: one for executing the code, and another one for measuring the memory consumption
%   - Start giving details about the first experiment we did to evaluate the difference in the ways we can measure memory consumption
%     - For such expeeriment, we started by using a single process
%     - We evaluated what was the memory usage reported for each method
%     - We used Envelope as our seismic attribute, using a randomly genereated input data of size 1000x1000x100
%   - Based on the results of the first experiment, we decided to use the /proc filesystem to measure memory consumption
%     - Not only because it was the most accurate, but also because it was easier to handle multiprocessing
%     - Also, considering the way we created the experiment, using the /proc is far easier to handle
%   - The complex part of using /proc is that we don't have a consolidated value for memory consumption
%   - To solve this, we used the smaps_rollup file and we executed an experiment to evaluate
%   - After this, we executed an experiment to get the memory consumption profile for all attributes
%     - We call iteration a combination of a given algorithm and an input shape
%     - For each iteration, we generate a new random data. We start with a shape of 200x200x200 and we increase the first dimension by 200 on every iteration
%     - Each shape is executed 5 times
%     - We do this until we reach the shape 10000x200x200
%     - Each iteration is executed in a separate Docker container
%     - The execution of an iteration is divided into the following steps:
%       - We first generate the random data
%       - Then we execute the attribute
%     - On each step, the experiment waits for a signal to continue. We have a shell script that is listening to the outputs of the experiment and everytime the experiment is waiting for a signal that script measures the memory consumption and sends the signal for the experiment to continue
%   - After we executed that experiment, we tried to evaluate manually if the values we collected were correct. While doing so we found out that the algorithm for a given shape was capable of executing even with less memory than it consumed
%   - We assume this happens due to the way Python garbage collection strategy
%   - Because of that, we executed the same experiment, but now trying to add memory pressure
%     - The structure is pretty similar, but for every iteration (a combination of shape and algorithm) we try adding memory pressure
%     - Adding memory pressure is basically reducing the amount of available memory on the container by a given percentage
%     - We did this with a step of 5% until the algorithm breaks and it isn't able to execute
%   - After all experiments, we evaluated the results against a real dataset
%   - We used F3 as our real dataset
%   - For this experiment, we executed both F3 and a synthetic dataset with the same shape to understand how much memory both of those consume
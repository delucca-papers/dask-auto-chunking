\chapter{Measuring Memory Consumption of Python Programs}
\label{ch:measuring-memory-consumption}


\section{Introduction}
\label{sec:mmc-introduction}

Accurately measuring the memory consumption of Python programs is a fundamental aspect of performance analysis, especially in the context of scientific computing and data analysis workflows.
Scientific applications often involve large-scale computations, high-dimensional datasets, and complex algorithms that necessitate the use of \ac{HPC} environments such as supercomputers and distributed systems.
In these environments, resource allocation is a critical factor, directly impacting both computational efficiency and cost-effectiveness.
Understanding the memory usage patterns of applications enables researchers and engineers to allocate appropriate resources, optimize computational workflows, and prevent system-level bottlenecks.

In scientific workflows, the need for precise memory measurement extends beyond mere resource allocation.
Memory profiling is instrumental in identifying inefficiencies, diagnosing performance issues, and ensuring the scalability of algorithms across diverse computational environments.
Although this thesis primarily focuses on the execution of scientific workflows rather than their optimization, the accurate measurement of memory consumption remains pivotal.
Without precise memory metrics, the evaluation of algorithmic performance and the reproducibility of experimental results can be significantly compromised.

\subsection{Factors Affecting Memory Consumption Evaluation}

The evaluation of memory consumption in Python applications is influenced by a multitude of factors spanning both the language's inherent characteristics and the underlying operating system's behavior.
Python, as a high-level language, abstracts many low-level memory management operations, introducing complexities that can obscure the true memory footprint of an application.

\begin{enumerate}
    \item \textbf{Dynamic Memory Allocation}:
    Python's memory model relies heavily on dynamic memory allocation, facilitated by its internal memory manager.
    The interpreter frequently allocates and deallocates memory for objects, leveraging techniques such as reference counting and cyclic garbage collection.
    These mechanisms introduce variability in memory usage, as memory may not be immediately released upon object deletion, leading to transient peaks in memory consumption.

    \item \textbf{Garbage Collection}:
    Python employs a \ac{GC} to manage memory, particularly for cyclic references that reference counting alone cannot handle.
    The \ac{GC} operates in generational cycles, triggering collections based on thresholds related to object allocations and deallocations.
    The timing of these collections can significantly affect memory profiling, as delayed garbage collection may artificially inflate memory usage metrics.

    \item \textbf{Memory Fragmentation}:
    Both Python's memory allocator (e.g., pymalloc~\cite{pymalloc}) and the underlying C libraries can cause memory fragmentation.
    Fragmentation leads to inefficient memory utilization, where the allocated memory space cannot be fully utilized due to non-contiguous free blocks.
    This phenomenon can result in higher apparent memory usage than the actual data footprint.

    \item \textbf{Operating System Optimizations}:
    Modern operating systems implement various memory management optimizations, such as virtual memory, \ac{COW}, and memory compression.
    The virtual memory system abstracts physical memory, allowing processes to perceive a contiguous memory space.
    \ac{COW} mechanisms, often triggered during process forking, can complicate memory measurements by deferring actual memory duplication until modification occurs.
    Additionally, features like Linux's zswap~\cite{zswap} and zram~\cite{zram} compress memory pages to optimize RAM usage, further obscuring accurate memory accounting.

    \item \textbf{Caching Mechanisms}:
    Python and the operating system both employ caching strategies that can skew memory usage metrics.
    For instance, Python maintains internal caches for frequently used objects (e.g., small integers and strings), while the OS uses disk and memory caches to optimize I/O operations.
    These caches can persist across program executions, leading to inconsistent memory profiles unless explicitly cleared.

    \item \textbf{Third-Party Libraries}:
    Scientific applications often rely on third-party libraries (e.g., NumPy~\cite{numpy}, pandas~\cite{pandas}, TensorFlow~\cite{tensorflow}), which may implement their own memory management strategies.
    These libraries, typically written in C or C++, interact directly with system memory, bypassing Python's garbage collector.
    Consequently, memory usage attributed to Python may not fully represent the actual consumption, necessitating specialized profiling tools to capture native allocations.
\end{enumerate}

\subsection{The Criticality of Precise Memory Measurement in Experiments}

Achieving precise memory measurement is a delicate and challenging task, particularly in experimental setups where even minor inconsistencies can introduce significant biases.
Inaccurate memory profiling can lead to erroneous conclusions about an algorithm's efficiency, scalability, and resource requirements.

One of the primary challenges is the accumulation of residual memory from previous executions.
This issue is especially pronounced in interactive environments like Jupyter~\cite{jupyter} notebooks, where code cells can be executed multiple times without restarting the kernel.
Each execution may leave behind allocated memory that is not immediately reclaimed, either due to lingering references or delayed garbage collection.
This residual memory inflates subsequent memory usage measurements, creating misleading results.

For example, consider a scenario where a data-intensive operation is repeatedly executed within a Jupyter~\cite{jupyter} notebook.
Even if the data structures are explicitly deleted using \texttt{del}, Python's \ac{GC} might not immediately free the associated memory, particularly if references exist in the notebook's global namespace or within closures.
Over time, this leads to cumulative memory bloat, distorting the true memory footprint of the operation.

Beyond interactive environments, process-level memory accumulation can occur when running batch scripts or automated workflows.
If multiple experiments are executed sequentially within the same process without proper memory isolation, residual allocations from earlier runs can affect subsequent measurements.
Techniques such as spawning isolated subprocesses for each experiment can mitigate this issue, ensuring clean memory states between runs.

Another critical consideration is the impact of measurement tools themselves.
Profiling tools, whether external (e.g., psutil~\cite{psutil}, resource~\cite{importlib_resources}) or internal (e.g., tracemalloc~\cite{tracemalloc}), introduce overhead that can influence the very metrics they aim to capture.
High-frequency sampling, for instance, increases \ac{CPU} and memory load, potentially skewing performance characteristics.
Moreover, tools that rely on instrumentation may alter code execution paths, subtly affecting memory allocation patterns.

To address these challenges, rigorous experimental protocols are essential.
This includes:

\begin{itemize}
    \item Isolating experiments in separate processes or containers to prevent cross-contamination of memory states.
    \item Resetting the environment (e.g., restarting the Python interpreter or Jupyter~\cite{jupyter} kernel) before each measurement to ensure a clean slate.
    \item Controlling external factors, such as disabling OS-level caches or running benchmarks on dedicated, idle systems to minimize background noise.
    \item Using hybrid measurement techniques, combining high-level process metrics with low-level memory tracing for comprehensive profiling.
\end{itemize}

In conclusion, precise memory measurement is not merely a technical detail but a cornerstone of robust experimental methodology.
It underpins the reliability and validity of performance evaluations, guiding both theoretical insights and practical optimizations in scientific computing.


\section{Approaches to Measure Memory Consumption}
\label{sec:mmc-approaches}

When measuring memory consumption in Python programs, it is essential to distinguish between two broad categories of approaches: external measurement techniques and internal measurement techniques.
This classification helps clarify the scope and effectiveness of different methods, as each group offers unique insights into memory usage from different perspectives.

\begin{itemize}
    \item \textbf{External Measurement Approaches}
    involve monitoring memory usage from outside the Python runtime environment.
    These techniques interact directly with the operating system to gather data about process memory consumption.
    They are particularly effective for capturing the overall memory footprint of a program, including memory allocated by external libraries, system-level resources, and native code executed alongside Python.
    Examples include tools like psutil~\cite{psutil}, the resource~\cite{importlib_resources} module, and accessing memory metrics from the Linux /proc~\cite{procfs} filesystem.

    \item \textbf{Internal Measurement Approaches},
    on the other hand, operate within the Python runtime itself.
    These methods provide fine-grained visibility into Python-specific memory allocations, allowing developers to trace memory usage down to individual objects, code lines, or function calls.
    They are invaluable for identifying memory leaks, inefficient data structures, and understanding memory behavior at a granular level.
    The tracemalloc~\cite{tracemalloc} module is a prime example of an internal measurement approach.
\end{itemize}

The separation into these two categories is not arbitrary; it reflects the complementary nature of the information provided.
External tools offer a high-level, system-wide view of memory consumption, which is crucial for understanding how a Python program interacts with the broader operating environment.
In contrast, internal tools delve deep into Python’s memory management, offering detailed diagnostics that external tools cannot capture.

Combining both external and internal measurement techniques often yields the most comprehensive understanding of a program’s memory behavior.
External tools can detect overall memory growth trends and system resource usage, while internal tools can pinpoint the specific code responsible for memory allocation.
This dual approach is especially useful in scientific computing workflows, where performance optimization requires both macro-level monitoring and micro-level diagnostics.

\subsection{External Measurement Approaches}

\subsubsection{psutil}

The psutil~\cite{psutil} library is a cross-platform tool that allows Python programs to access system details and process information, including memory usage.
It interacts with the operating system to retrieve real-time data about process memory, \ac{CPU} usage, and other system metrics.

psutil~\cite{psutil} works by interfacing with system APIs, such as /proc~\cite{procfs} on Linux, the Windows \ac{API} on Windows systems, and system calls on \ac{macOS}.
It can report key memory metrics like:

\begin{itemize}
    \item \textbf{\ac{RSS}}:
    The portion of memory occupied by a process that is held in \ac{RAM}.

    \item \textbf{\ac{VMS}}:
    The total amount of virtual memory used by the process.

    \item \textbf{Shared Memory}:
    Memory shared with other processes.
\end{itemize}

psutil~\cite{psutil} provides a comprehensive overview of system-wide and process-specific resource usage, making it a versatile tool for performance monitoring.

\subsubsection{resource Module}

The resource~\cite{importlib_resources} module is part of Python's standard library and offers a lightweight method to track resource usage of the current process.
It provides metrics such as maximum \ac{RSS}, which reflects the peak physical memory usage during the execution of the program.

Unlike psutil~\cite{psutil}, which can monitor ongoing memory usage, resource is primarily used for capturing peak memory usage at specific checkpoints in the code.
This makes it ideal for benchmarking and performance evaluations.

\subsubsection{/proc Filesystem}

On Linux systems, the /proc~\cite{procfs} virtual filesystem provides detailed information about processes, including memory usage.
Accessing \texttt{/proc/[pid]/smaps} allows for fine-grained analysis of memory allocation.

The smaps file breaks down memory usage into categories such as:

\begin{itemize}
    \item \textbf{Private and Shared Memory}:
    Differentiates between private memory and memory shared with other processes.

    \item \textbf{Anonymous Pages}:
    Memory not backed by any file.

    \item \textbf{Heap and Stack Segments}:
    Detailed insights into dynamic memory allocations.
\end{itemize}

While powerful, this approach requires root access for detailed process information and involves parsing raw text files, which can add complexity.

\subsection{Internal Measurement Approaches}

\subsubsection{tracemalloc}

tracemalloc~\cite{tracemalloc} is a built-in Python module designed for tracing memory allocations.
It hooks into Python's memory allocator to record the size and source of each allocation, capturing stack traces for memory usage hotspots.

Key features of tracemalloc~\cite{tracemalloc} include:

\begin{itemize}
    \item \textbf{Memory Snapshots}:
    Capturing snapshots of memory usage at different points in the program's execution.

    \item \textbf{Snapshot Comparisons}:
    Analyzing changes in memory usage between snapshots.

    \item \textbf{Tracking Allocation Sources}:
    Identifying the specific lines of code responsible for memory usage.

    \item \textbf{Filtering and Grouping}:
    Aggregating memory statistics based on filenames, line numbers, or function calls.
\end{itemize}

While tracemalloc~\cite{tracemalloc} introduces some overhead, its granularity makes it invaluable for debugging memory-related issues within Python applications.


\section{Materials and Methods}
\label{sec:mmc-materials-methods}

\DFTODO{Explicar sobre o setup do experimento de um ponto de vista de hardware}
\DFTODO{Explicar sobre o setup dos experimentos de um ponto de vista de software}
\DFTODO{Descrever as estratégias adotadas para garantir a precisão das medições, como isolar cada experimento em processos separados para evitar vazamentos de memória e garantir ambientes limpos.}
\DFTODO{Explicar o fluxo do experimento}
\DFTODO{Explicar os algoritmos utilizados}
\DFTODO{Explicar as métricas coletadas: tempo de execução e uso máximo de memória}
\DFTODO{Explicar as ferramentas utilizadas para medir as métricas}
\DFTODO{Explicar a estratégia de análise de dados das métricas que serão feitas}
\DFTODO{Apresentar e linkar o código fonte dos experimentos}


\section{Experimental Results}
\label{sec:mmc-results}

\DFTODO{Demonstrar resultados dos experimentos com viés (sem controlar o ambiente). Mostrando o impacto de execuções anteriores}
\DFTODO{Apresentar os resultados dos experimentos, mostrando o consumo de memória e o tempo de execução para cada algoritmo.}
\DFTODO{Destacar a validação feita para garantir a precisão das medições. Encontrando a questão da pressão de memória}
\DFTODO{Demonstrar o impacto ao aplicar pressão de memória}


\section{Conclusion}
\label{sec:mmc-conclusion}

\DFTODO{Concluir a respeito da necessidade de ambientes controlados para medir a memória}
\DFTODO{Concluir sobre as diferentes formas de medir a memória}
\DFTODO{Concluir sobre o garbage collector e a possibilidade de executar com pressão de memória}
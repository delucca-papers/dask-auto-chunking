\subsection{Seismic Data Iteration Design}

The primary objective of this research was to compile a comprehensive profile of the memory consumption for each seismic attribute.
Achieving this required executing the same attribute on seismic datasets of varying shapes.

Understanding the nature of seismic datasets is crucial, as these datasets are typically three-dimensional, including dimensions such as inline, crossline, and time or depth slices.
These dimensions reflect different aspects of the seismic survey grid, where inline and crossline indicate horizontal measurements, and the third dimension represents vertical depth or time.

To systematically study the impact of dataset dimension variation, the research strategy involved adjusting the sizes of seismic datasets while maintaining a consistent structure, as outlined in Table \ref{table:iteration-design}.
The study fixed the sizes of the latter two dimensions, usually corresponding to crossline and time/depth, at 200.
This fixed size was chosen based on the evaluation that varying a single dimension would have similar memory usage impacts, irrespective of the dimension altered.
Thus, changing the inline dimension would consume an amount of memory comparable to altering other dimensions.

Systematically varying the first dimension in increments of 200 enabled the exploration of a spectrum of dataset sizes, from relatively small to larger and more complex ones.
Incrementing the size in a controlled fashion allowed for a detailed observation of changes in memory consumption, providing insights into how scalability and dataset dimensions influence the memory footprint of seismic data processing.

Each experiment iteration executed the same seismic attribute processing task on a dataset with a varied first dimension size, while the other two dimensions remained constant.
This method offered a nuanced profile of memory consumption across different dataset sizes, yielding important findings on the efficiency and scalability of seismic data processing algorithms within high-performance computing environments.
The detailed iteration design aimed to clarify how different dataset shapes impact memory usage, a crucial factor for optimizing seismic data processing tasks in large-scale geological explorations.

\begin{table}[htbp]
    \centering
    \begin{threeparttable}
        \caption{Iteration Design for Synthetic Data}
        \label{table:iteration-design}
        \begin{tabular}{|c|c|c|c|}
            \hline
            \textbf{Iteration} & \textbf{Inline} & \textbf{Crossline} & \textbf{Time/Depth} \\
            \hline
            1                  & 200             & 200                & 200                 \\
            \hline
            2                  & 400             & 200                & 200                 \\
            \hline
            3                  & 600             & 200                & 200                 \\
            \hline
            \vdots             & \vdots          & \vdots             & \vdots              \\
            \hline
            n-1                & (n-1) * 200     & 200                & 200                 \\
            \hline
            n                  & n * 200         & 200                & 200                 \\
            \hline
        \end{tabular}
        \begin{tablenotes}
            \small
            \item In each iteration, the dimensions of the synthetic seismic dataset are specified.
            The "Inline" dimension increases by steps of 200, while the "Crossline" and "Time/Depth" dimensions remain constant at 200.
        \end{tablenotes}
    \end{threeparttable}
\end{table}